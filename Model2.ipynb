{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4abc2e2c-e555-4ec0-8b86-d36e9c746aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# importing dependencies here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# visualizations\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# feature engineering\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# sentiment scoring\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# pos tagging\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# accuracy scores\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    ")\n",
    "\n",
    "# performance check\n",
    "import time\n",
    "\n",
    "# sparse to dense\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "\n",
    "class DenseTransformer(TransformerMixin):\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "\n",
    "# importing model\n",
    "from joblib import load\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdcbfa50-f0f9-4fa1-ae34-3310214f3396",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_csv = (\"df_holdout.csv\")\n",
    "df = pd.read_csv(path_to_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fea52140-cf83-4474-8c72-93767c46451b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'I have never seen so many poorly used memes.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'Wow! You are obviously her muse... Be flatter...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  'I have never seen so many poorly used memes.....\n",
       "1  INFJ  'Wow! You are obviously her muse... Be flatter..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa0fc3a6-4ddc-45f4-b17d-35feace78841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_types(personality_data):\n",
    "    personality_data[\"is_Extrovert\"] = personality_data[\"type\"].apply(\n",
    "        lambda x: 1 if x[0] == \"E\" else 0\n",
    "    )\n",
    "    personality_data[\"is_Sensing\"] = personality_data[\"type\"].apply(\n",
    "        lambda x: 1 if x[1] == \"S\" else 0\n",
    "    )\n",
    "    personality_data[\"is_Thinking\"] = personality_data[\"type\"].apply(\n",
    "        lambda x: 1 if x[2] == \"T\" else 0\n",
    "    )\n",
    "    personality_data[\"is_Judging\"] = personality_data[\"type\"].apply(\n",
    "        lambda x: 1 if x[3] == \"J\" else 0\n",
    "    )\n",
    "\n",
    "    # rearranging the dataframe columns\n",
    "    personality_data = personality_data[\n",
    "        [\"type\", \"is_Extrovert\", \"is_Sensing\", \"is_Thinking\", \"is_Judging\", \"posts\"]\n",
    "    ]\n",
    "\n",
    "\"\"\"\n",
    "def clean_posts(personality_data):\n",
    "    # converting posts into lower case\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"posts\"].str.lower()\n",
    "\n",
    "    # replacing ||| with space\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].str.replace(\n",
    "        r\"\\|\\|\\|\", \" \"\n",
    "    )\n",
    "\n",
    "    # replacing urls with domain name\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].str.replace(\n",
    "        r\"https?:\\/\\/(www)?.?([A-Za-z_0-9-]+)([\\S])*\",\n",
    "        \"\"\n",
    "    )\n",
    "\n",
    "    # dropping emails\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].str.replace(\n",
    "        r\"\\S+@\\S+\", \"\"\n",
    "    )\n",
    "\n",
    "    # dropping punctuations\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].str.replace(\n",
    "        r\"[^a-z\\s]\", \" \"\n",
    "    )\n",
    "\n",
    "    # dropping MBTIs mentioned in the posts\n",
    "    mbti = personality_data[\"type\"].unique()\n",
    "    for type_word in mbti:\n",
    "        personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].str.replace(\n",
    "            type_word.lower(), \"\"\n",
    "        )\n",
    "        \n",
    "    # removing words that are 1 to 2 characters long    \n",
    "    personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].str.replace(\n",
    "        r\"\\b\\w{1,2}\\b\", \"\"\n",
    "    )\n",
    "\n",
    "    # lemmitizing\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].apply(\n",
    "        lambda x: \" \".join(\n",
    "            [\n",
    "                lemmatizer.lemmatize(word)\n",
    "                for word in x.split(\" \")\n",
    "                if word not in stopwords.words(\"english\")\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # tag_posts will be a list of 50 lists.\n",
    "    # replacing urls with domain name\n",
    "    personality_data[\"tag_posts\"] = personality_data[\"posts\"].str.replace(\n",
    "        r\"https?:\\/\\/(www)?.?([A-Za-z_0-9-]+)([\\S])*\",\n",
    "        lambda match: match.group(2),\n",
    "    )\n",
    "    # replacing ||| with space\n",
    "    personality_data[\"tag_posts\"] = [\n",
    "        post for post in personality_data[\"tag_posts\"].str.split(\"\\|\\|\\|\")\n",
    "    ]\n",
    "\"\"\"\n",
    "\n",
    "def clean_posts(personality_data):\n",
    "    # converting posts into lower case\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"posts\"].str.lower()\n",
    "\n",
    "    # replacing ||| with space\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].str.replace(\n",
    "        r\"\\|\\|\\|\", \" \"\n",
    "    )\n",
    "\n",
    "    # replacing urls with domain name\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].str.replace(\n",
    "        r\"https?:\\/\\/(www)?.?([A-Za-z_0-9-]+)([\\S])*\",\n",
    "        lambda match: match.group(2) if match.group(2) is not None else \"\",\n",
    "        regex=True\n",
    "    )\n",
    "\n",
    "    # dropping emails\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].str.replace(\n",
    "        r\"\\S+@\\S+\", \"\"\n",
    "    )\n",
    "\n",
    "    # dropping punctuations\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].str.replace(\n",
    "        r\"[^a-z\\s]\", \" \"\n",
    "    )\n",
    "\n",
    "    # dropping MBTIs mentioned in the posts\n",
    "    mbti = personality_data[\"type\"].unique()\n",
    "    for type_word in mbti:\n",
    "        personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].str.replace(\n",
    "            type_word.lower(), \"\"\n",
    "        )\n",
    "        \n",
    "    # removing words that are 1 to 2 characters long    \n",
    "    personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].str.replace(\n",
    "        r\"\\b\\w{1,2}\\b\", \"\"\n",
    "    )\n",
    "\n",
    "    # lemmitizing\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    personality_data[\"clean_posts\"] = personality_data[\"clean_posts\"].apply(\n",
    "        lambda x: \" \".join(\n",
    "            [\n",
    "                lemmatizer.lemmatize(word)\n",
    "                for word in x.split(\" \")\n",
    "                if word not in stopwords.words(\"english\")\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # tag_posts will be a list of 50 lists.\n",
    "    # replacing urls with domain name\n",
    "    personality_data[\"tag_posts\"] = personality_data[\"posts\"].str.replace(\n",
    "        r\"https?:\\/\\/(www)?.?([A-Za-z_0-9-]+)([\\S])*\",\n",
    "        lambda match: match.group(2) if match.group(2) is not None else \"\",\n",
    "        regex=True\n",
    "    )\n",
    "    # replacing ||| with space\n",
    "    personality_data[\"tag_posts\"] = [\n",
    "        post for post in personality_data[\"tag_posts\"].str.split(\"\\|\\|\\|\")\n",
    "    ]\n",
    "\n",
    "\n",
    "def sentiment_score(personality_data):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    nlp_sentiment_score = []\n",
    "\n",
    "    for post in personality_data[\"clean_posts\"]:\n",
    "        score = analyzer.polarity_scores(post)[\"compound\"]\n",
    "        nlp_sentiment_score.append(score)\n",
    "\n",
    "    personality_data[\"compound_sentiment\"] = nlp_sentiment_score\n",
    "\n",
    "\n",
    "def pos_tagging(personality_data):\n",
    "    personality_data[\"tagged_words\"] = personality_data[\"tag_posts\"].apply(\n",
    "        lambda x: [nltk.pos_tag(word_tokenize(line)) for line in x]\n",
    "    )\n",
    "\n",
    "    # grouping pos tags based on stanford list\n",
    "    tags_dict = {\n",
    "        \"ADJ\": [\"JJ\", \"JJR\", \"JJS\"],\n",
    "        \"ADP\": [\"EX\", \"TO\"],\n",
    "        \"ADV\": [\"RB\", \"RBR\", \"RBS\", \"WRB\"],\n",
    "        \"CONJ\": [\"CC\", \"IN\"],\n",
    "        \"DET\": [\"DT\", \"PDT\", \"WDT\"],\n",
    "        \"NOUN\": [\"NN\", \"NNS\", \"NNP\", \"NNPS\"],\n",
    "        \"NUM\": [\"CD\"],\n",
    "        \"PRT\": [\"RP\"],\n",
    "        \"PRON\": [\"PRP\", \"PRP$\", \"WP\", \"WP$\"],\n",
    "        \"VERB\": [\"MD\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"],\n",
    "        \".\": [\"#\", \"$\", \"''\", \"(\", \")\", \",\", \".\", \":\"],\n",
    "        \"X\": [\"FW\", \"LS\", \"UH\"],\n",
    "    }\n",
    "\n",
    "    def stanford_tag(x, tag):\n",
    "        tags_list = [len([y for y in line if y[1] in tags_dict[col]]) for line in x]\n",
    "        return tags_list\n",
    "\n",
    "    for col in tags_dict.keys():\n",
    "        personality_data[col + \"_avg\"] = personality_data[\"tagged_words\"].apply(\n",
    "            lambda x: np.mean(stanford_tag(x, col))\n",
    "        )\n",
    "\n",
    "\n",
    "def get_counts(personality_data):\n",
    "    def unique_words(s):\n",
    "        unique = set(s.split(\" \"))\n",
    "        return len(unique)/50\n",
    "\n",
    "    def emojis(post):\n",
    "        emoji_count = 0\n",
    "        words = post.split()\n",
    "        for e in words:\n",
    "            if \"http\" not in e:\n",
    "                if e.count(\":\") == 2:\n",
    "                    emoji_count += 1\n",
    "        return emoji_count/50\n",
    "\n",
    "    def colons(post):\n",
    "        colon_count = 0\n",
    "        words = post.split()\n",
    "        for e in words:\n",
    "            if \"http\" not in e:\n",
    "                colon_count += e.count(\":\")\n",
    "        return colon_count/50\n",
    "\n",
    "    personality_data[\"qm\"] = personality_data[\"posts\"].apply(lambda s: s.count(\"?\")/50)\n",
    "    personality_data[\"em\"] = personality_data[\"posts\"].apply(lambda s: s.count(\"!\")/50)\n",
    "    personality_data[\"colons\"] = personality_data[\"posts\"].apply(colons)\n",
    "    personality_data[\"emojis\"] = personality_data[\"posts\"].apply(emojis)\n",
    "\n",
    "    personality_data[\"word_count\"] = personality_data[\"posts\"].apply(\n",
    "        lambda s: (s.count(\" \") + 1)/50\n",
    "    )\n",
    "    personality_data[\"unique_words\"] = personality_data[\"posts\"].apply(unique_words)\n",
    "\n",
    "    personality_data[\"upper\"] = personality_data[\"posts\"].apply(\n",
    "        lambda x: len([x for x in x.split() if x.isupper()])/50\n",
    "    )\n",
    "    personality_data[\"link_count\"] = personality_data[\"posts\"].apply(\n",
    "        lambda s: s.count(\"http\")/50\n",
    "    )\n",
    "    ellipses_count = [\n",
    "        len(re.findall(r\"\\.\\.\\.\\ \", posts))/50 for posts in personality_data[\"posts\"]\n",
    "    ]\n",
    "    personality_data[\"ellipses\"] = ellipses_count\n",
    "    personality_data[\"img_count\"] = [\n",
    "        len(re.findall(r\"(\\.jpg)|(\\.jpeg)|(\\.gif)|(\\.png)\", post))/50\n",
    "        for post in personality_data[\"posts\"]\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66aa6346-8fa5-49ad-a494-8c5fe892709a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(personality_data):\n",
    "\n",
    "    t = time.time()\n",
    "\n",
    "    categorize_types(personality_data)\n",
    "\n",
    "    clean_posts(personality_data)\n",
    "\n",
    "    sentiment_score(personality_data)\n",
    "\n",
    "    pos_tagging(personality_data)\n",
    "\n",
    "    get_counts(personality_data)\n",
    "\n",
    "    features = personality_data[\n",
    "        [\n",
    "            \"clean_posts\",\n",
    "            \"compound_sentiment\",\n",
    "            \"ADJ_avg\",\n",
    "            \"ADP_avg\",\n",
    "            \"ADV_avg\",\n",
    "            \"CONJ_avg\",\n",
    "            \"DET_avg\",\n",
    "            \"NOUN_avg\",\n",
    "            \"NUM_avg\",\n",
    "            \"PRT_avg\",\n",
    "            \"PRON_avg\",\n",
    "            \"VERB_avg\",\n",
    "            \"qm\",\n",
    "            \"em\",\n",
    "            \"colons\",\n",
    "            \"emojis\",\n",
    "            \"word_count\",\n",
    "            \"unique_words\",\n",
    "            \"upper\",\n",
    "            \"link_count\",\n",
    "            \"ellipses\",\n",
    "            \"img_count\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    X = features\n",
    "    y = personality_data.iloc[:, 2:6]\n",
    "\n",
    "    print(f\"Total Preprocessing Time: {time.time()-t} seconds\\n\")\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b60f4f2-a478-4c76-8385-2f5d5a8a7f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_classes(y_pred1, y_pred2, y_pred3, y_pred4):\n",
    "    \n",
    "    combined = []\n",
    "    for i in range(len(y_pred1)):\n",
    "        combined.append(\n",
    "            str(y_pred1[i]) + str(y_pred2[i]) + str(y_pred3[i]) + str(y_pred4[i])\n",
    "        )\n",
    "    \n",
    "    result = trace_back(combined)\n",
    "    return result\n",
    "    \n",
    "\n",
    "def trace_back(combined):\n",
    "        \n",
    "    type_list = [\n",
    "    {\"0\": \"I\", \"1\": \"E\"},\n",
    "    {\"0\": \"N\", \"1\": \"S\"},\n",
    "    {\"0\": \"F\", \"1\": \"T\"},\n",
    "    {\"0\": \"P\", \"1\": \"J\"},\n",
    "    ]\n",
    "\n",
    "    result = []\n",
    "    for num in combined:\n",
    "        s = \"\"\n",
    "        for i in range(len(num)):\n",
    "            s += type_list[i][num[i]]\n",
    "        result.append(s)\n",
    "        \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1aa64ca-2fbd-445f-aa2f-d5e428779aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(path_to_csv):\n",
    "\n",
    "    df = pd.read_csv(path_to_csv)\n",
    "\n",
    "    X, y = prep_data(df)\n",
    "\n",
    "    # loading the 4 models\n",
    "    EorI_model = load(\"clf_is_Extrovert.joblib\")\n",
    "    SorN_model = load( \"clf_is_Sensing.joblib\")\n",
    "    TorF_model = load(\"clf_is_Thinking.joblib\")\n",
    "    JorP_model = load(\"clf_is_Judging.joblib\")\n",
    "\n",
    "    # predicting\n",
    "    EorI_pred = EorI_model.predict(X)\n",
    "    print(\n",
    "        \"Extrovert vs Introvert Accuracy: \",\n",
    "        accuracy_score(y[\"is_Extrovert\"], EorI_pred),\n",
    "    )\n",
    "    print(\"y_true\", y[\"is_Extrovert\"].values)\n",
    "    print(\"preds\", EorI_pred)\n",
    "\n",
    "    SorN_pred = SorN_model.predict(X)\n",
    "    print(\n",
    "        \"\\nSensing vs Intuition Accuracy: \", accuracy_score(y[\"is_Sensing\"], SorN_pred)\n",
    "    )\n",
    "    print(\"y_true\", y[\"is_Sensing\"].values)\n",
    "    print(\"preds\", SorN_pred)\n",
    "\n",
    "    TorF_pred = TorF_model.predict(X)\n",
    "    print(\n",
    "        \"\\nThinking vs Feeling Accuracy: \", accuracy_score(y[\"is_Thinking\"], TorF_pred)\n",
    "    )\n",
    "    print(\"y_true\", y[\"is_Thinking\"].values)\n",
    "    print(\"preds\", TorF_pred)\n",
    "\n",
    "    JorP_pred = JorP_model.predict(X)\n",
    "    print(\n",
    "        \"\\nJudging vs Perceiving Accuracy: \", accuracy_score(y[\"is_Judging\"], JorP_pred)\n",
    "    )\n",
    "    print(\"y_true\", y[\"is_Judging\"].values)\n",
    "    print(\"preds\", JorP_pred)\n",
    "\n",
    "    # combining the predictions from the 4 models\n",
    "    result = combine_classes(EorI_pred, SorN_pred, TorF_pred, JorP_pred)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "310eb3c5-4c46-43d9-bfa1-acc221ccc10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Preprocessing Time: 30.219797611236572 seconds\n",
      "\n",
      "Extrovert vs Introvert Accuracy:  0.6206896551724138\n",
      "y_true [0 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0\n",
      " 0 1 1 0 1 0 0 0 0 0 0 0 0]\n",
      "preds [1 1 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0\n",
      " 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 0 0 0 0 1 0 1 1 0\n",
      " 1 1 1 0 1 0 0 0 1 1 0 0 0]\n",
      "\n",
      "Sensing vs Intuition Accuracy:  0.6551724137931034\n",
      "y_true [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 1 0 1 0 0 0 1]\n",
      "preds [0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 1 0 0 0 1 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1\n",
      " 0 0 1 0 0 1 0 0 0 1 1 0 1]\n",
      "\n",
      "Thinking vs Feeling Accuracy:  0.7931034482758621\n",
      "y_true [0 0 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 0 0 0\n",
      " 0 0 1 1 1 0 0 0 0 0 0 0 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 1\n",
      " 1 0 1 1 1 0 1 0 1 1 0 1 1]\n",
      "preds [0 0 1 1 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0 0 1 1 1 1 1 1 0 0 0\n",
      " 0 0 1 1 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 0 0 0 1 0 1\n",
      " 1 0 1 1 1 0 0 0 1 1 0 0 0]\n",
      "\n",
      "Judging vs Perceiving Accuracy:  0.6666666666666666\n",
      "y_true [1 1 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 1 0 0\n",
      " 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 1 0 0 1\n",
      " 1 0 1 0 0 0 1 1 0 1 1 0 1]\n",
      "preds [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0\n",
      " 1 0 0 1 0 0 0 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 0 0 1 1 1 1 1 1 0 1 1 0 1 0 1\n",
      " 1 1 1 0 0 0 0 0 1 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    predictions = predict(\"df_holdout.csv\")\n",
    "    y_truth = pd.read_csv(\"df_holdout.csv\")[\"type\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3efbec06-611b-464f-bedb-022c4a33aa50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'I have never seen so many poorly used memes.....</td>\n",
       "      <td>ENFP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'Wow! You are obviously her muse... Be flatter...</td>\n",
       "      <td>ENFP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENFP</td>\n",
       "      <td>'Are you shitting me? He's so type 7 its not e...</td>\n",
       "      <td>ESTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'Oh man, this is serious. Good luck with her! ...</td>\n",
       "      <td>INTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>INFP</td>\n",
       "      <td>'Haha. Thank you! You guys have been so nice. ...</td>\n",
       "      <td>ENFP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>'Hey, so incidentally, that's the exact same t...</td>\n",
       "      <td>ENTJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'I would answer your questions but I don't kno...</td>\n",
       "      <td>ESTJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'I am the same. What I do,  is send them occas...</td>\n",
       "      <td>ISFJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Important that i'm attracted to her at least ...</td>\n",
       "      <td>INFP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>ISTJ</td>\n",
       "      <td>This thread is dead but, watching the show som...</td>\n",
       "      <td>ISFJ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>87 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    type                                              posts result\n",
       "0   INFJ  'I have never seen so many poorly used memes.....   ENFP\n",
       "1   INFJ  'Wow! You are obviously her muse... Be flatter...   ENFP\n",
       "2   ENFP  'Are you shitting me? He's so type 7 its not e...   ESTP\n",
       "3   ENTP  'Oh man, this is serious. Good luck with her! ...   INTP\n",
       "4   INFP  'Haha. Thank you! You guys have been so nice. ...   ENFP\n",
       "..   ...                                                ...    ...\n",
       "82  ISTP  'Hey, so incidentally, that's the exact same t...   ENTJ\n",
       "83  INTJ  'I would answer your questions but I don't kno...   ESTJ\n",
       "84  INFJ  'I am the same. What I do,  is send them occas...   ISFJ\n",
       "85  INTP  'Important that i'm attracted to her at least ...   INFP\n",
       "86  ISTJ  This thread is dead but, watching the show som...   ISFJ\n",
       "\n",
       "[87 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"result\"] = predictions\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bce964e-d9c0-42e7-b85c-7103d36ca950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That somehow managed to be record short yet answer almost all the questions we would've asked, haha! Hi Deb! Welcome to Hou Tian; nice to meet you! I'm Jhenne, one of the mods here-- which means I gotta give you the modly speech :] Make sure to check out the Mandatory Reading up top! Our constantly updated Library is also a great resource, though it isn't mandatory reading-- we like to tell members to 'read as you need', rather than marathon read it all at once. One of the most helpful threads is the Gameplay So Far thread, which breaks down what all has gone down on the boards. (Now, the summary for January isn't tossed up yet, but we'd be happy to break down what you missed if you'd like.) I see that you're interested in Mai! That means both the Trying for a Canon Character page, and the Canon Character Rules and Consequences post will be helpful to check out. If you're ever considering an original character, we have our player-made adoptables list, and our factions, comprised of the Jade Shark/Bending Opposition, Original People of the Flame, and The Bending Crime Syndicates. As far as characters go, in the past tense I play Srai, a Jade Shark [s]that is very very dusty. In the Korraverse I play a reporter named Chihiro, and an ex-taxi dancer/wannabe actress named Naoki, and a Republic City University student named Haruna. I think that's it! If you have any questions, don't hesitate to ask a mod, or drop it right here in this thread so we can get back to you! Again, welcome! #CONFETTI\n",
      "                                         clean_posts  compound_sentiment  \\\n",
      "0  hat somehow managed record short yet answer al...              0.9834   \n",
      "\n",
      "   ADJ_avg  ADP_avg  ADV_avg  CONJ_avg  DET_avg  NOUN_avg  NUM_avg  PRT_avg  \\\n",
      "0       27       10       19        29       30        66        1        6   \n",
      "\n",
      "   ...  qm  em  colons  emojis  word_count  unique_words  upper  link_count  \\\n",
      "0  ...   0   8       1       0         271           180      6           0   \n",
      "\n",
      "   ellipses  img_count  \n",
      "0         0          0  \n",
      "\n",
      "[1 rows x 22 columns]\n",
      "Preprocessing Time: 0.19404172897338867 seconds\n"
     ]
    }
   ],
   "source": [
    "mbti = [\n",
    "    \"INFP\",\n",
    "    \"INFJ\",\n",
    "    \"INTP\",\n",
    "    \"INTJ\",\n",
    "    \"ENTP\",\n",
    "    \"enfp\",\n",
    "    \"ISTP\",\n",
    "    \"ISFP\",\n",
    "    \"ENTJ\",\n",
    "    \"ISTJ\",\n",
    "    \"ENFJ\",\n",
    "    \"ISFJ\",\n",
    "    \"ESTP\",\n",
    "    \"ESFP\",\n",
    "    \"ESFJ\",\n",
    "    \"ESTJ\",\n",
    "]\n",
    "tags_dict = {\n",
    "    \"ADJ_avg\": [\"JJ\", \"JJR\", \"JJS\"],\n",
    "    \"ADP_avg\": [\"EX\", \"TO\"],\n",
    "    \"ADV_avg\": [\"RB\", \"RBR\", \"RBS\", \"WRB\"],\n",
    "    \"CONJ_avg\": [\"CC\", \"IN\"],\n",
    "    \"DET_avg\": [\"DT\", \"PDT\", \"WDT\"],\n",
    "    \"NOUN_avg\": [\"NN\", \"NNS\", \"NNP\", \"NNPS\"],\n",
    "    \"NUM_avg\": [\"CD\"],\n",
    "    \"PRT_avg\": [\"RP\"],\n",
    "    \"PRON_avg\": [\"PRP\", \"PRP$\", \"WP\", \"WP$\"],\n",
    "    \"VERB_avg\": [\"MD\", \"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"],\n",
    "    \".\": [\"#\", \"$\", \"''\", \"(\", \")\", \",\", \".\", \":\"],\n",
    "    \"X\": [\"FW\", \"LS\", \"UH\"],\n",
    "}\n",
    "features = [\n",
    "    \"clean_posts\",\n",
    "    \"compound_sentiment\",\n",
    "    \"ADJ_avg\",\n",
    "    \"ADP_avg\",\n",
    "    \"ADV_avg\",\n",
    "    \"CONJ_avg\",\n",
    "    \"DET_avg\",\n",
    "    \"NOUN_avg\",\n",
    "    \"NUM_avg\",\n",
    "    \"PRT_avg\",\n",
    "    \"PRON_avg\",\n",
    "    \"VERB_avg\",\n",
    "    \"qm\",\n",
    "    \"em\",\n",
    "    \"colons\",\n",
    "    \"emojis\",\n",
    "    \"word_count\",\n",
    "    \"unique_words\",\n",
    "    \"upper\",\n",
    "    \"link_count\",\n",
    "    \"ellipses\",\n",
    "    \"img_count\",\n",
    "]\n",
    "\n",
    "\n",
    "def unique_words(s):\n",
    "    unique = set(s.split(\" \"))\n",
    "    return len(unique)\n",
    "\n",
    "\n",
    "def emojis(post):\n",
    "    # does not include emojis made purely from symbols, only :word:\n",
    "    emoji_count = 0\n",
    "    words = post.split()\n",
    "    for e in words:\n",
    "        if \"http\" not in e:\n",
    "            if e.count(\":\") == 2:\n",
    "                emoji_count += 1\n",
    "    return emoji_count\n",
    "\n",
    "\n",
    "def colons(post):\n",
    "    # Includes colons used in emojis\n",
    "    colon_count = 0\n",
    "    words = post.split()\n",
    "    for e in words:\n",
    "        if \"http\" not in e:\n",
    "            colon_count += e.count(\":\")\n",
    "    return colon_count\n",
    "\n",
    "\n",
    "def lemmitize(s):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    new_s = \"\"\n",
    "    for word in s.split(\" \"):\n",
    "        lemmatizer.lemmatize(word)\n",
    "        if word not in stopwords.words(\"english\"):\n",
    "            new_s += word + \" \"\n",
    "    return new_s[:-1]\n",
    "\n",
    "\n",
    "def clean(s):\n",
    "    # remove urls\n",
    "    s = re.sub(re.compile(r\"https?:\\/\\/(www)?.?([A-Za-z_0-9-]+).*\"), \"\", s)\n",
    "    # remove emails\n",
    "    s = re.sub(re.compile(r\"\\S+@\\S+\"), \"\", s)\n",
    "    # remove punctuation\n",
    "    s = re.sub(re.compile(r\"[^a-z\\s]\"), \"\", s)\n",
    "    # Make everything lowercase\n",
    "    s = s.lower()\n",
    "    # remove all personality types\n",
    "    for type_word in mbti:\n",
    "        s = s.replace(type_word.lower(), \"\")\n",
    "    return s\n",
    "\n",
    "\n",
    "def prep_counts(s):\n",
    "    clean_s = clean(s)\n",
    "    d = {\n",
    "        \"clean_posts\": lemmitize(clean_s),\n",
    "        \"link_count\": s.count(\"http\"),\n",
    "        \"youtube\": s.count(\"youtube\") + s.count(\"youtu.be\"),\n",
    "        \"img_count\": len(re.findall(r\"(\\.jpg)|(\\.jpeg)|(\\.gif)|(\\.png)\", s)),\n",
    "        \"upper\": len([x for x in s.split() if x.isupper()]),\n",
    "        \"char_count\": len(s),\n",
    "        \"word_count\": clean_s.count(\" \") + 1,\n",
    "        \"qm\": s.count(\"?\"),\n",
    "        \"em\": s.count(\"!\"),\n",
    "        \"colons\": colons(s),\n",
    "        \"emojis\": emojis(s),\n",
    "        \"unique_words\": unique_words(clean_s),\n",
    "        \"ellipses\": len(re.findall(r\"\\.\\.\\.\\ \", s)),\n",
    "    }\n",
    "    return clean_s, d\n",
    "\n",
    "\n",
    "def prep_sentiment(s):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    score = analyzer.polarity_scores(s)\n",
    "    d = {\n",
    "        \"compound_sentiment\": score[\"compound\"],\n",
    "        \"pos_sentiment\": score[\"pos\"],\n",
    "        \"neg_sentiment\": score[\"neg\"],\n",
    "        \"neu_sentiment\": score[\"neu\"],\n",
    "    }\n",
    "    return d\n",
    "\n",
    "\n",
    "def tag_pos(s):\n",
    "    tagged_words = nltk.pos_tag(word_tokenize(s))\n",
    "    d = dict.fromkeys(tags_dict, 0)\n",
    "    for tup in tagged_words:\n",
    "        tag = tup[1]\n",
    "        for key, val in tags_dict.items():\n",
    "            if tag in val:\n",
    "                tag = key\n",
    "        d[tag] += 1\n",
    "    return d\n",
    "\n",
    "\n",
    "def prep_data(s):\n",
    "    clean_s, d = prep_counts(s)\n",
    "    d.update(prep_sentiment(lemmitize(clean_s)))\n",
    "    d.update(tag_pos(clean_s))\n",
    "    return pd.DataFrame([d])[features]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = time.time()\n",
    "    string = \"That somehow managed to be record short yet answer almost all the questions we would've asked, haha! Hi Deb! Welcome to Hou Tian; nice to meet you! I'm Jhenne, one of the mods here-- which means I gotta give you the modly speech :] Make sure to check out the Mandatory Reading up top! Our constantly updated Library is also a great resource, though it isn't mandatory reading-- we like to tell members to 'read as you need', rather than marathon read it all at once. One of the most helpful threads is the Gameplay So Far thread, which breaks down what all has gone down on the boards. (Now, the summary for January isn't tossed up yet, but we'd be happy to break down what you missed if you'd like.) I see that you're interested in Mai! That means both the Trying for a Canon Character page, and the Canon Character Rules and Consequences post will be helpful to check out. If you're ever considering an original character, we have our player-made adoptables list, and our factions, comprised of the Jade Shark/Bending Opposition, Original People of the Flame, and The Bending Crime Syndicates. As far as characters go, in the past tense I play Srai, a Jade Shark [s]that is very very dusty. In the Korraverse I play a reporter named Chihiro, and an ex-taxi dancer/wannabe actress named Naoki, and a Republic City University student named Haruna. I think that's it! If you have any questions, don't hesitate to ask a mod, or drop it right here in this thread so we can get back to you! Again, welcome! #CONFETTI\"\n",
    "    print(string)\n",
    "    print(prep_data(string))\n",
    "    print(f\"Preprocessing Time: {time.time() - t} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edb47534-f645-4d36-ac70-c349f12a1bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted personality type: INTP\n",
      "INTP\n",
      "Preprocessing Time: 0.84 seconds\n"
     ]
    }
   ],
   "source": [
    "def predict(s):\n",
    "    return len(s.split(\" \"))\n",
    "\n",
    "\n",
    "def predict_e(s):\n",
    "    X = prep_data(s)\n",
    "\n",
    "    EorI_model = load(\"clf_is_Extrovert.joblib\")\n",
    "    SorN_model = load(\"clf_is_Sensing.joblib\")\n",
    "    TorF_model = load(\"clf_is_Thinking.joblib\")\n",
    "    JorP_model = load(\"clf_is_Judging.joblib\")\n",
    "\n",
    "    EorI_pred = EorI_model.predict(X)[0]\n",
    "    SorN_pred = SorN_model.predict(X)[0]\n",
    "    TorF_pred = TorF_model.predict(X)[0]\n",
    "    JorP_pred = JorP_model.predict(X)[0]\n",
    "\n",
    "    # Mapping binary predictions to MBTI letters\n",
    "    EorI_type = \"E\" if EorI_pred == 1 else \"I\"\n",
    "    SorN_type = \"S\" if SorN_pred == 1 else \"N\"\n",
    "    TorF_type = \"T\" if TorF_pred == 1 else \"F\"\n",
    "    JorP_type = \"J\" if JorP_pred == 1 else \"P\"\n",
    "\n",
    "    mbti_type = EorI_type + SorN_type + TorF_type + JorP_type\n",
    "    print(\"Predicted personality type:\", mbti_type)\n",
    "\n",
    "    return mbti_type\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = time.time()\n",
    "    # sample test string. Type ISTP.\n",
    "    string = \"I plugged the data into tableau to see how the different features or how various mathematical formulas relate to the Weight. Once I had a few that didn’t have a wide distribution, I just started trying different models, even ones we hadn’t gone over yet. There are a LOT of regression models. I do not like this try everything method, it’s inefficient and illogical.\"\n",
    "    print(predict_e(string))\n",
    "    print(f\"Preprocessing Time: {(time.time() - t):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "464a38b0-75e3-400c-a552-8f3ccbd802b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted personality type: INTP\n",
      "INTP\n",
      "Preprocessing Time: 1.17 seconds\n"
     ]
    }
   ],
   "source": [
    "def predict_e(s):\n",
    "    X = prep_data(s)\n",
    "\n",
    "    EorI_model = load(\"clf_is_Extrovert.joblib\")\n",
    "    SorN_model = load(\"clf_is_Sensing.joblib\")\n",
    "    TorF_model = load(\"clf_is_Thinking.joblib\")\n",
    "    JorP_model = load(\"clf_is_Judging.joblib\")\n",
    "\n",
    "    EorI_pred = EorI_model.predict(X)[0]\n",
    "    SorN_pred = SorN_model.predict(X)[0]\n",
    "    TorF_pred = TorF_model.predict(X)[0]\n",
    "    JorP_pred = JorP_model.predict(X)[0]\n",
    "\n",
    "    # Mapping binary predictions to MBTI letters\n",
    "    EorI_type = \"E\" if EorI_pred == 1 else \"I\"\n",
    "    SorN_type = \"S\" if SorN_pred == 1 else \"N\"\n",
    "    TorF_type = \"T\" if TorF_pred == 1 else \"F\"\n",
    "    JorP_type = \"J\" if JorP_pred == 1 else \"P\"\n",
    "\n",
    "    mbti_type = EorI_type + SorN_type + TorF_type + JorP_type\n",
    "    print(\"Predicted personality type:\", mbti_type)\n",
    "\n",
    "    return mbti_type\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t = time.time()\n",
    "    string = \"I plugged the data into tableau to see how the different features or how various mathematical formulas relate to the Weight. Once I had a few that didn’t have a wide distribution, I just started trying different models, even ones we hadn’t gone over yet. There are a LOT of regression models. I do not like this try everything method, it’s inefficient and illogical.\"\n",
    "    print(predict_e(string))\n",
    "    print(f\"Preprocessing Time: {(time.time() - t):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c428821-ccda-426f-804a-2769de872776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted personality type: INFP\n",
      "INFP\n",
      "Processing Time: 0.74 seconds\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    t = time.time()\n",
    "    #string = \"Hi everyone! I'm feeling energized today and excited to connect with all of you. I love being around people and making meaningful connections. Empathy and understanding are important values to me, and I always strive to support and uplift others. Let's work together to create positive change and make a difference in the world!\"\n",
    "    #string = \"Why’s she here?Ah yes but what do I use for the rod. Those two cunts aren’t coming out.Forget what happened. Focus on the journey now.Everything’s alright with you dw about that , it’s okay to get mad. There’s no reason to get triggered like this Control your anger.You’re fluctuating way too much these days. Let’s just not jump in too much tho, can’t be perfect right? I get that classes are empty but there’s no point in shifting all 120 guys over there. idk not much work no? So there’s no point in finishing it immediately no?\"\n",
    "    #string = \"For the current 6th ( going to 7th) a company is ready to give many internship in  AI applied to  cyber security provided the students knoNote that 'python' is not equal to ML.  ML= Mathsw basic maths.Institute project is about applications of LLM. Need lots of data preparation job. Those who are really hardworking and show real attitude, we will recommend to companies where Amrita Alumnus are leading the AI Software  development.|||SVD,  Pseudoinverse and ADMM are the three pillars of AI and Datascience. We expect all to master this and can be asked  in any viva ( in any subject). No 'A' grade without proper knowledge of these three topics.There will be a workshop on RDkit for bio and chemo_informatics. Knowledge in many domains (like above+NLP+Vision+ +finance+robotics etc) is needed to get a job.\"\n",
    "    #string = \"I've been reflecting a lot on how we can make the world a kinder place, and I think small acts of compassion can create a ripple effect of positive change. Sometimes, it's the little things that make the biggest difference in someone's day. I love finding unique ways to express my creativity, whether it's through writing, art, or simply daydreaming about new ideas. It's important for me to stay true to my values and be authentic in everything I do. I believe that if we all listened to our hearts more, the world would be a more harmonious place\"\n",
    "    string = \"I guess yeh life long chalega you know||| Theres this one fine looking dude i saw here tho||| Lets go hyderabdi hunting ||| Dude that thing was on 31 next day after i came there wasnt time to ||| Ok question is just where do we drink ||| Like meko akele toh obviosly nahi pina h ||| i dont like cheese ||| I dont talk alot.\"\n",
    "    #string =  \"Dear Dr. Ramesh Sivan Pillai from University of Wyoming USA is giving a 1 hour ta\n",
    "    print(predict_e(string))\n",
    "    print(f\"Processing Time: {(time.time() - t):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea3b2ee-b057-4985-a10f-8e1f20149b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
